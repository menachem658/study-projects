---
title: "lab 4 machine learning"
author: "Menachem Sokolik & valeria lerman"
date: "20 6 2021"
output:
    rmarkdown::github_document:
    theme: journal
    toc: true
    toc_depth: 3
    df_print: paged
---
```{r, cache=TRUE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

knitr::opts_chunk$set(warning=FALSE)
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # This includes dplyr, stringr, ggplot2, ..
library(data.table)
set.seed(2)
```

### 1-3
Loading and understanding our data.
```{r}
library(ISLR)
full_dat <- NCI60
?NCI60
```

What is the format of the data? Notice that NCI60 is a list containing two
elements – data and labels

NCI microarray data. The data contains expression levels on 6830 genes from 64 cancer cell lines. Cancer type is also recorded.
The format is a list containing two elements: data and labs.

data is a 64 by 6830 matrix of the expression values while labs is a vector listing the cancer types for the 64 cell lines.

What is The size of the data?
```{r}
dim <- dim(full_dat$data)
cat(paste0("In our data table there are ", dim[1], paste0(" rows"), paste0(" and ", dim[2], paste0(" columns."))))
```
Useing the str function in order to take a glimpse at the data.
```{r}
str(full_dat$data)
```
We can see that we have 64 vectors which contain 6830 observations

How many different labels are there? Use table to learn about the different
labels you have (the labels are stored in full_dat$labs)
```{r}
knitr::kable(table(full_dat$labs))
cat(paste0("In our data we have ", dim(table(full_dat$labs)), paste0(" different labels")))
```

### PCA
#### a.
Run PCA using the "prcomp" function.
```{r}
data <- full_dat$data
fit.pca <- prcomp(data,scale=T) # PCA and scaling
```

It is very important to scale the variables if they are measured in different units and hence having different variances. Typically, when we perform PCA on unscaled variables, the first principal component will have a very large loading for the variable with the largest variance.

#### b.
Explore the PCA object – What does it contain? 
What is the meaning of each value?
```{r}
knitr::kable(names(fit.pca), , col.names = NULL, caption = "name")
?prcomp
```

prcomp - returns a list with class "prcomp" containing the following components:

sdev - the standard deviations of the principal components (i.e., the square roots of the eigenvalues of the covariance/correlation matrix, though the calculation is actually done with the singular values of the data matrix).

rotation	- the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors). The function princomp returns this in the element loadings.

x - if retx is true the value of the rotated data (the centred (and scaled if requested) data multiplied by the rotation matrix) is returned. Hence, cov(x) is the diagonal matrix diag(sdev^2). For the formula method, napredict() is applied to handle the treatment of values omitted by the na.action.

center, scale - the centering and scaling used, or FALSE.

#### c.
Extract the standard deviation of each of the PCS. 
How much of the variance is explained by each of the PCS?
```{r}
sd <- fit.pca$sdev
var.explained <- sd^2 # variance explained
p.var.explained <- as.data.frame(round(var.explained/sum(var.explained),3)) #  normalizing by the sum of all the PCs' variance
```

#### d.
Calculate the cumulative proportion of variance explained by the PCS. How many PCS are needed to explain 50%/75%/100% of the variance in the data?
```{r}
cpvar <- cumsum(data.frame(cp=c(1:64),`prop`=round(p.var.explained*100,3)))
cpvar$cp <- c(1:64)
cpvar

exd.data <- cpvar %>% filter(cp== 12|cp== 28|cp== 63)
names(exd.data) <- c("Principal Components","proportion")
exd.data
```

As we can see, 12 components give us the cumulative explained variance of 50% 28 components give the cumulative explained variance of 75% and 63 components give the cumulative variance of 100%, we can also understand it, because we know that the highest explained variance comes from n-1 components (when n is the original number of components).


#### e.
Plot the cumulative proportion of variance explained by the PCs as a function of the number of PCs (i.e. the cumulative sum on the y axis and the number of PCs on the x axis)
```{r}
plot(cpvar$cp, cpvar$round.var.explained.sum.var.explained...3.,xlab="PC", ylab= "variance",main="cumulative proportion of variance explained by the PC", pch=3)
```


#### f.
Let's see how the data looks in the PC space.
Plot the PC scores each observation got on the first two PCs.
```{r}
plot(fit.pca$x[,1],fit.pca$x[,2], xlab = "PC1", ylab="PC2", main = "The first two PCA of cancer type", pch=8)
```

We can see from the plot, that the largest explained variance is placed on the horizontal line between $(0,20)$ PC2 values, and the orthogonal vertical line with the second largest variance is located around $(-20,0)$ value. 

#### g.
Run the following code in order to generate a different color for each label type, and color the observations with the color of their label
```{r}
cols <- rainbow(length(unique(full_dat$labs)))
obs_cols <- cols[as.numeric(as.factor(full_dat$labs))]

plot(fit.pca$x[,1],fit.pca$x[,2], xlab = "PC1", ylab="PC2", main = "The first Two PCA of cancer type", col= obs_cols, pch=11)
```

Note that each color represents a different group in our plot.

### Hierarchical Clustering
**Hierarchical clustering is an approach which does not require a particular choice of k.**
*It has an advantage over k-means that it results in a tree-based representation called a dendogram*

The method produces hierarchical representations in which the clusters at each level of the hierarchy are created by merging clusters at the next lower level.
At the lowest level each cluster contains a single observation.
At the highest level there is only one cluster containing all of the data.

#### a.
Scale the data in order for all the variables to have the same scale.
```{r}
sc_data <- scale(data)
```
It is very important to scale the variables if they are measured in different units and hence having different variances. Typically, when we perform PCA on unscaled variables, the first principal component will have a very large loading for the variable with the largest variance.

#### b.
Use the dist function in order to calculate the distances between every two observations.
```{r}
dist <- dist(sc_data)
```
In order to decide which clusters should be combined a measure of dissimilarity, distance metric, between sets of observations is required.
The distance metric will greatly influence the formation of the clusters.

#### c.
Run hierarchical clustering using complete linkage.
```{r}
hc_fit <- hclust(dist,method = "complete")
```
we need to specify the linkage criteria which determines the distance between sets of observations as a function of the pairwise distances between observations.
complete linkage = $d_{CL}(G,H)=max_{i\in G,j\in H}d_{i,j}$

#### d.
Plot the dendrogram of the model
```{r}
plot(hc_fit,labels = full_dat$labs, main ="complete linkage", xlab="", ylab="",cex=.5, legend=NULL,hang = -1)
```


#### e + f.
how the model divides the observations into these clusters.
We chose randomly k = 7

```{r}
library(factoextra)


fviz_nbclust(sc_data, FUN=hcut, method="silhouette")

cutting <- cutree(hc_fit,2)
plot(hc_fit,labels = full_dat$labs,cex=0.5,hang = -1)
rect.hclust(hc_fit, k=7, border = 1:7)
abline(h=140,col='purple')
```

We use function "rect.hclust" to Draw Rectangles Around Hierarchical Clusters (Draws rectangles around the branches of a dendrogram highlighting the corresponding clusters. First the dendrogram is cut at a certain level, then a rectangle is drawn around selected branches.).

It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value (i.e., maximum value) of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters.

We have divided into seven sections which each existing section has a common denominator and is obtained by dividing "complete".
For example four on the left can be said to be "melanoma" cancer, and section three is "nsclc" and on that way.



### K-Means

When we cluster observations of a dataset, we seek to
partition them into distinct groups so that the observations
within each group are quit similar and observations in different
groups are quit different. We need to define what it means to be similar which is often domain-specific.
In K-mean clustering we partition the observations into a
pre-specified number of clusters.
one can cluster features based on the
observations in order to discover subgroups among the
features.

#### a.
Run kmeans on the scaled data from 5a. and set the k it should use.
```{r}
fviz_nbclust(sc_data, FUN=kmeans, method="silhouette")
k_mean <- kmeans(sc_data, 2)
```
This function dertemines and visualize the optimal number of clusters using different methods: within cluster sums of squares, average silhouette and gap statistics.
We used this function in order to find the optimal number of clusters.
According to the graph above, that the Average silhouette width is the highest for k=2, and almost as good for k=5, so the function returns k=2.

#### b.
Explore the resulting k-means object – What did the function return? What does each of its values?
```{r}
df <- data.frame(values=names(k_mean))
df
```
cluster:  vector of integer numbers that show for every point which cluster it's allocated to

centers: matrix of cluster centers
it is also equal to the mean value of all the points that are allocated to the cluster

totss: total sum of squares by definition

withinss: within cluster sum of squares by definition

tot.withinss: sum of withinss by definition

betweenss: between cluster sum of squares, also equal to the difference between totss and tot.withinss

size: number of points in a current cluster

iter: number of iterations

ifault: indicator of an algorithmichal problem which may appear

#### c.
How much are the clusters diverse?
i. Extract the within-cluster sum of squares.
```{r}
k_mean$withinss
```
The within-point scatter can be written as:
$\sum _{k=1}^k\:\:n_k\sum \:_{c\left(i\right)=k}^{ }\:\:\left|| x_i-\overline{x_k}\right||^2$

ii. Extract the between-cluster sum of squares
```{r}
k_mean$betweenss
```
The similarity measure for any two observations is defined by
the squared Euclidean distance.

#### d.
Extract the clusters of each observation
```{r}
data.frame(cluster=k_mean$cluster)
```


Return to the plot from 4f. Color the observations using the k-means
clusters. Hint: set col=km$cluster+1

```{r}
par(mfrow=c(1,2))
plot(fit.pca$x[,1],fit.pca$x[,2], xlab = "PC1", ylab="PC2", main = "PCA", pch=8, col=obs_cols)

plot(fit.pca$x[,1],fit.pca$x[,2], xlab = "PC1", ylab="PC2", main = "K means", pch=8, col=k_mean$cluster+1)
```

We can see that after using the k-means coloring, the right plot shows the exact devision to clusters of all the observations (according to the optimal number of clustering we found above).
On the left side, the observations are not clustered.