---

title: '52414: Home Exam'

output:

  html_document: default

  pdf_document: default

date: "July 18th, 2021"

---







### Q0.Submission Instructions (Please read carefully)   



The exam will be submitted **individually** by uploading the solved exam `Rmd` and `html` files to the course `moodle`. 

Please name your files as `52414-HomeExam_ID.Rmd` and `52414-HomeExam_ID.html` where `ID` is replaced by your ID number (do **not** write your name in the file name or in the exam itself).

The number of points for each sub-question is indicated next to it, with $105$ points overall. The total grade will be at most $100$. 



Once you click on the `moodle` link for the home exam, the exam will start and you have three days (72 hours) to complete and submit it. 

The exam will be available from July 18th to July 30th. The last submission time is June 30th at 23:59. <br>

You may use all course materials, the web and other written materials and R libraries. 

You are NOT allowed to discuss any of the exam questions/materials with other students. 





**Analysis and Presentation of Results:**



Write your answers and explanations in the text of the `Rmd` file (*not* in the `code`). <br>

The text of your answers should be next to the relevant code, plots and tables and refer to them, and not at a separate place at the end. <br>

You need to explain every step of your analysis. When in doubt, a more detailed explanation is better than omitting explanations. 



Give informative titles, axis names and names for each curve/bar in your graphs. 

In some graphs you may need to change the graph limits. If you do so, please include the outlier points you have removed in a separate table.  <br>

Add informative comments explaining your code <br>



Whenever possible, use *objective* and *specific* terms and quantities learned in class, and avoid *subjective* and *general* unquantified statements. For example: <br>

`Good:` "We see a $2.5$-fold increase in the curve from Jan. 1st to March 1st". <br>

`Bad:` "The curve goes up at the beginning". <br>

`Good:` "The median is $4.7$. We detected five outliers with distance $>3$ standard deviations from the median". <br>

`Bad:` "The five points on the sides seem far from the middle". 



Sometimes `Tables` are the best way to present your results (e.g. when asked for a list of items). Exclude irrelevant

rows/columns. Display clearly items' names in your `Tables`.



Show numbers in plots/tables using standard digits and not scientific display. 

That is: 90000000 and not 9e+06.  

Round numbers to at most 3 digits after the dot - that is, 9.456 and not 9.45581451044



Some questions may require data wrangling and manipulation which you need to 

decide on. The instructions may not specify precisely the exact plot you should use

(for example: `show the distribution of ...`). In such cases, you should decide what and how to show the results. 



When analyzing real data, use your best judgment if you encounter missing values, negative values, NaNs, errors in the data etc. (e.g. excluding them, zeroing negative values..) and mention what you have done in your analysis in such cases. 



Required libraries are called in the `Rmd` file. Install any library missing from your `R` environment. You are allowed to add additional libraries if you want. 

If you do so, *please add them at the start of the Rmd file, right below the existing libraries, and explain what libraries you've added, and what is each new library used for*. 



##############################################################################











<br/><br/>







## Q1. Two Armies Simulation (45 pt)    

<img src="https://images.freeimages.com/images/premium/previews/1923/19232816-toy-soldiers-war-concepts.jpg" alt="soldiers" width="300"/>



Consider two armies of $10$ `R` loving statisticians and $10$ `Python` loving statisticians, facing each other in a shootout, fighting to the death over which language is better. 



Once the battle starts, assume that each statistician tries to shoot as fast as she can, where the time until shooting has an exponential distribution with $\lambda=1$. After a shot is fired, the statistician keeps firing, with the time to the next shot again distributed as $exp(1)$. Each statistician keeps shooting until she is shot and killed herself by a statistician from the opposing army, and leaves the battle. The times until shooting the next bullet for all statisticians and all shots are independent. <br>

At each shot, the statistician chooses as target **uniformly at random** a member from the remaining **living members** of the opposing army. 

<br>

The battle keeps going until all persons from one of the armies die, and then the other army is declared the `winner`. 

Let $X$ be the number of remaining statisticians from the `winner` army when the battle ends. <br>

Throughout this question, assume that statisticians are **perfect shooters**, and always hit their target (the choice of the target changes however between different sub-questions below).





a. (5pt) Describe in words a simulation strategy to estimate $E[X]$ and $Var(X)$, including how would you simulate a battle between the two armies. <br>

**Hint:** remember that the exponential distribution has a memoryless property: $Pr(T>t) = Pr(T > t+s | T>s)$, $\forall t, s > 0$. <br>

You can perform the simulations in this question exactly as described, which may take many minutes to run, or perform **simpler** and **faster** simulations using probabilistic arguments, provided that they are **equivalent** to the description in the question. <br>

(For example, if you were requested to simulate $n$ i.i.d. $Bernouli(p)$ random variables and report their sum, you could argue that instead it is enough to simulate a single $Bionomial(n,p)$ random variable).





b. (8pt) Simulate $1,000$ random battles as described in the question and use them to estimate $E[X]$ and $Var(X)$ from the random simulations.  <br>

It is recommended to write a function for the simulation and call it, such that the simulation function can be used also in the subsequent sub-questions. 


c. (8pt) Now, change $n$, the number of statisticians in each army, to be $n=10, 20, 40, ..., 10240$ (each time multiplying $n$ by two), and let $X_n$ be the random variable counting the number of remaining winners when starting with $n$ statisticians in each army. (so the variable $X$ from (a.) corresponds to $X_{10}$). <br>

For each value of $n$ simulate $100$ random battles and estimate $\mu_n \equiv E[X_n]$. 

Plot your estimate vs. $n$. <br>

Find a simple function $f(n)$ such that it holds that $\mu_n \approx f(n)$ based on the plot. 

(**Hint:** you can use log-scale). 





d. (8pt) In this sub-question, assume that all statisticians in both armies have used their programming language too much so they became to hate it, and therefore in each shot they aim and kill a random member from their **own** army (including possibly themselves). <br>

Modify the simulation to accommodate this case, and repeat the simulation, plot and finding a function $f(n)$ as in (c.) for this case. <br>

Explain in words the differences in results between the two cases. 





e. (8pt) In this sub-question, assume that all statisticians in both armies are **completely drunk**, and shoot randomly one of the **remaining persons alive** (from both armies) including themselves (they still always hit their target).  

Repeat (d.) for this case. Are the results similar or different? why? 





f. (8pt) Finally, suppose in this sub-question that statisticians that are shot become zombies instead of being killed, and can still keep shooting at statisticians from the opposing army (as in (a.), (b.)). <br>

All statisticians aim at and hit a random **living** (non-zombie) member from the opposing army. The battle ends when all members of a certain army become zombies, and then $X_n$ records the number of remaining living (non-zombie) statisticians in the other army. <br>

Repeat the simulation, plot and finding a function $f(n)$ as in (c.) for this case. <br>

Explain in words the differences in results between the this and the previous cases. 





**Solutions:**

### Q0.Submission Instructions (Please read carefully).  

a. (5pt) Describe in words a simulation strategy to estimate $E[X]$ and $Var(X)$, including how would you simulate a battle between the two armies.

In order to find the expectation I will first define the problem. There are two vectors, each of which has ten statisticians. Each statistician's firing speed is divided by $exp(1)$. To calculate the expectation we will first understand how the war works. The shooter with the lowest firing time rate will hit starters and reduce the opposing team man randomly (evenly distributed distribution). After the shooting I will add to the shooter in $exp(1)$ for the time he had before. Repeating this process as the required number in the simulation, I will add the number of survivors in the remaining group a new vector (res), on which I will calculate the expectation and variance.
The expectation is striving for the average all by the law of large numbers.
The variance is calculated as the difference from the mean squared.

b. (8pt) Simulate $1,000$ random battles as described in the question and use them to estimate $E[X]$ and $Var(X)$ from the random simulations.

Building a function according to the motivation described in section a.
First we find the person with the lowest shooting rate and which group he is from. Then, we will randomly subtract a person from the opposing team (a person chooses in a uniformly distributed manner who to shoot.) And we will also add to the shooter the new shooting time rate divided by $exp(1)$.We will continue until there are no people in one group at all.
We will add the surviving locator survivors at all run times.


```r
battle.simulation = function(n){
  team.1 <- rexp(n,1)
  team.2 <- rexp(n,1)
  while(length(team.1) > 0 & length(team.2) > 0){
    if (min(team.1) < min(team.2)){
      x <- rdunif(1,1,length(team.2))
      team.2 <- team.2[-x]
      team.1[which.min(team.1)] <- team.1[which.min(team.1)] + rexp(1)}
    else{x <- rdunif(1,1,length(team.1))
      team.1 <- team.1[-x]
      team.2[which.min(team.2)] <- team.2[which.min(team.2)] + rexp(1)}}
  return(max(length(team.1),length(team.2)))}
```

Running the simulation $1000$ teims when $n=10$ in each team.

```r
res <- c()
for(i in 1:1000){res[i] <- battle.simulation(10)}
mean.ex1.b <- mean(res) # calculation of the mean.
var.ex1.b <- var(res) # calculation of the var.
mean.var <- data.frame(mean=round(mean.ex1.b,3),var=round(var.ex1.b,3))
knitr::kable(mean.var, caption = "Mean & Var")
```



Table: Mean & Var

|  mean|   var|
|-----:|-----:|
| 5.793| 4.939|

c. (8pt) Now, change $n$, the number of statisticians in each army, to be $n=10, 20, 40, ..., 10240$.


```r
n <- unlist(lapply(0:10, function(i) c(rep(10*2^i)))) # vector of n
E.x <- c() # vector of means
res.ex1.c <- c()
for(i in 1:length(n)){
  res.ex1.c = sapply(1:100, function(index){res.ex1.c[index] <- battle.simulation(n[i])})
  E.x[i] <-mean(res.ex1.c)}

data.E.x <- data.frame(n = n, mean = E.x)
```


```r
P <- ggplot(data= data.E.x, aes(x=n,y=mean)) + geom_line() +
    geom_point() + theme_light() + labs(title="1.c Plot of estimate vs. n.") + xlab("n") + ylab("Mean")
P
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-5-1.png" width="672" />

It can be seen that when we do scale to the data then we get a linear line. A scale is made so that trends can be seen $y=log\left(x\right)$.

```r
P.1 <- ggplot(data= data.E.x, aes(x=n,y=mean)) + geom_line() +
    geom_point() + scale_x_log10() + scale_y_log10() + theme_light() + labs(title="1.c Plot of estimate vs. n with scale.") + xlab("n") + ylab("Mean")
P.1
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-6-1.png" width="672" />

d. (8pt) In this sub-question, assume that all statisticians in both armies have used their programming language too much so they became to hate it, and therefore in each shot they aim and kill a random member from their **own** army (including possibly themselves).

I changed the fact that the shooter was hurting his team.

```r
battle.simulation.d = function(n){
  team.1 <- rexp(n,1)
  team.2 <- rexp(n,1)
  while(length(team.1) > 0 & length(team.2) > 0){
    if (min(team.1) < min(team.2)){x <- rdunif(1,1,length(team.1)) # random member from their own
      team.1 <- team.1[-x] 
      team.1[which.min(team.1)] <- team.1[which.min(team.1)] + rexp(1)}
    else{x <- rdunif(1,1,length(team.2))
      team.2 <- team.2[-x]
      team.2[which.min(team.2)] <- team.2[which.min(team.2)] + rexp(1)}}
  return(max((length(team.1)),length(team.2)))}
```


```r
res.ex1.d <- c()
for(i in 1:1000){res.ex1.d[i] <- battle.simulation.d(10)}
mean.ex1.d <- mean(res.ex1.d) # calculation of the mean.
var.ex1.d <- var(res.ex1.d) # calculation of the var.
mean.var <- data.frame(mean=round(mean.ex1.d,3),var=round(var.ex1.d,3))
knitr::kable(mean.var, caption = "Mean & Var")
```



Table: Mean & Var

|  mean|   var|
|-----:|-----:|
| 1.634| 0.867|


```r
E.x.d <- c() # vector of means
res.ex1.d <- c()
for(i in 1:length(n)){
  E.x.d[i] = mean(sapply(1:100, function(index){res.ex1.d[index] <- battle.simulation.d(n[i])}))}

E.x.ex1.d <- data.frame(n = n, mean = E.x.d)
```


```r
P.d <- ggplot(data= E.x.ex1.d, aes(x=n,y=mean)) + geom_line() +
    geom_point() + theme_light() + labs(title="1.d Plot of estimate vs. n.") + xlab("n") + ylab("Mean")
P.d
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-10-1.png" width="672" />

In this quastion people are shooting only in their own team, so the teams doesnt have interaction between themself and the teams have the same shooting ratio ($exp(1)$ distribution). from that reason we would expect that the results will be similar, in the graph we can see that the difference is between $~0.817$ as we expect.Since variance is small then this is a fixed function.

e. (8pt) In this sub-question, assume that all statisticians in both armies are **completely drunk**, and shoot randomly one of the **remaining persons alive** (from both armies) including themselves (they still always hit their target).  

```r
battle.simulation.e = function(n){
  team.1 <- rexp(n,1)
  team.2 <- rexp(n,1)
  while(length(team.1) > 0 & length(team.2) > 0){
    if (min(team.1) < min(team.2)){
      team.1[which.min(team.1)] <- team.1[which.min(team.1)] + rexp(1)}
    else{team.2[which.min(team.2)] <- team.2[which.min(team.2)] + rexp(1)}
       x <- rdunif(1,1,length(team.1)+length(team.2))
    if(x>length(team.1)){x <- x-length(team.1) # the conditions that can shoot in everyone.
      team.2 <- team.2[-x]}
    else{team.1 <- team.1[-x]}}
  return(max(length(team.1),length(team.2)))}
```



```r
res.ex1.e <- c()
for(i in 1:1000){res.ex1.e[i] <- battle.simulation.e(10)}
mean.ex1.e <- mean(res.ex1.e) # calculation of the mean.
var.ex1.e <- var(res.ex1.e) # calculation of the var.
mean.var <- data.frame(mean=round(mean.ex1.e,3),var=round(var.ex1.e,3))
knitr::kable(mean.var, caption = "Mean & Var")
```



Table: Mean & Var

|  mean|   var|
|-----:|-----:|
| 1.811| 1.212|


```r
E.x.e <- c() # vector of means
res.ex1.e <- c()
for(i in 1:length(n)){
  res.ex1.e = sapply(1:100, function(index){res.ex1.e[index] <- battle.simulation.e(n[i])})
  E.x.e[i] <-mean(res.ex1.e)}

data.E.x.e <- data.frame(n = n, mean = E.x.e)
```


```r
P.e <- ggplot(data= data.E.x.e, aes(x=n,y=mean)) + geom_line() +
    geom_point() + theme_light() + labs(title="1.e Plot of estimate vs. n.") + xlab("n") + ylab("Mean")
P.e
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-14-1.png" width="672" />

Are the results similar or different? why?
It can be shown that they are similar in its trend that are not a function known to us, since this state is similar to a previous state. Anyone can kill from both groups so it is done as if there is one group that the chance of death ratio is maintained when the group grows in potential for death, but there is also more choice of who to shoot. Then the result is that you will enjoy more variety in this situation.

f. (8pt) Finally, suppose in this sub-question that statisticians that are shot become zombies instead of being killed, and can still keep shooting at statisticians from the opposing army (as in (a.), (b.)).


Explain in words the differences in results between the this and the previous cases.

```r
battle.simulation.f = function(n){
  count.1 <- 0 
  count.2 <- 0
  team.1 <- rexp(n,1)
  team.2 <- rexp(n,1)
  while(count.1 < n & count.2 < n){
    if (min(team.1) < min(team.2)){count.2 <- count.2 + 1 # the conditions that become zombies.
      team.1[which.min(team.1)] <- team.1[which.min(team.1)] + rexp(1)}
    else{count.1 <- count.1 + 1 # the conditions that become zombies.
      team.2[which.min(team.2)] <- team.2[which.min(team.2)] + rexp(1)}}
  return(n - min(count.1,count.2))} # the alive
```


```r
res.ex1.f <- c()
for(i in 1:1000){res.ex1.f[i] <- battle.simulation.f(10)}
mean.ex1.f <- mean(res.ex1.f) # calculation of the mean.
var.ex1.f <- var(res.ex1.f) # calculation of the var.
mean.var <- data.frame(mean=round(mean.ex1.f,3),var=round(var.ex1.f,3))
knitr::kable(mean.var, caption = "Mean & Var")
```



Table: Mean & Var

|  mean|  var|
|-----:|----:|
| 3.413| 3.81|


```r
E.x.f <- c() # vector of means
res.ex1.f <- c()
for(i in 1:length(n)){
  res.ex1.f = sapply(1:100, function(index){res.ex1.f[index] <- battle.simulation.f(n[i])})
  E.x.f[i] <-mean(res.ex1.f)}

data.E.x.f <- data.frame(n = n, mean = E.x.f)
```


```r
P.f <- ggplot(data= data.E.x.f, aes(x=n,y=mean)) + geom_line() +
    geom_point() + theme_light() + labs(title="1.f Plot of estimate vs. n with scale.") + xlab("n") + ylab("Mean")
P.f
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-18-1.png" width="672" />

It is necessary to make a certain transformation since the function is close to a known function $y=x^{1/2}$, So I will make this change.
The difference is that there are more people who can shoot, i.e. the shooters do not die but turn into zombies, and can shoot, so coming out of smaller origins from section A also in variance but not by much as the number of players increases then the difference between the two increases.


```r
P.1.f <- ggplot(data= data.E.x.f, aes(x=n,y=mean^1.655)) + geom_line() +
    geom_point()  + theme_light() + labs(title="1.f Plot of estimate vs. n with scale.") + xlab("n") + ylab("Mean")
P.1.f
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-19-1.png" width="672" />









## Q2. Analysis and Visualization of Twitter Data (60 pt)    



<img src="https://cdn-0.therandomvibez.com/wp-content/uploads/2018/12/Jokes-On-New-Years-Resolution.jpg" alt="resolutions" width="300"/>





a. (4pt) Download and read the tweets dataset file `New-years-resolutions-DFE.csv` available [here](https://github.com/DataScienceHU/DataAnalysisR_2021/blob/master/New-years-resolutions-DFE.csv). 

The data represents new year's resolutions tweets by American users wishing to change something in their life at the start of the year $2015$, downloaded from [here](https://data.world/crowdflower/2015-new-years-resolutions#). <br>

Make sure that the tweets `text` column has `character` type. 

Show the top and bottom two rows of the resulting data-frame. 





b. (5pt) The class `times` from the library `chron` stores and displays times in the above format `Hours:Minutes:Seconds`, but also treats them as numeric values between zero and one in units of days. For example, the time `10:48:00` corresponds to the value: $(10 + 48/60)/24 = 0.45$. <br>

Create a new column with tweet times, of class `times`, with the time of the day for each tweet, in the above format. For example, the first entry in the column corresponding to the time of the first tweet should be: `10:48:00`. <br>

Make a histogram showing the number of tweets in every hour of the $24$ hours in a day (that is, the bins are times between `00:00` and `00:59`, between `01:00` and `01:59` etc.). <br>

At which hours do we see the most/fewest tweets?





c. (6pt) Plot the distribution of tweets `text` lengths (in characters) made by `females` and `males` separately. Who writes longer tweets? <br>

Repeat, but this time plot the tweets lengths distribution for tweets in the four different regions of the US

(`Midwest`, `Northeast`, `South` and `West`). Report the major differences in lengths between regions. <br>

Finally, show the tweets lengths distribution for tweets for the $10$ different categories given in `Resolution_Category`. Report the major differences in lengths between categories. 





d. (8pt) Compute the number of occurrences of each word in the `text` of all the tweets. Ignore upper/lower case differences. <br>

Remove all common stop words (use the command `stop_words` from the tidytext package). <br>

Remove words containing the special characters: `#`, `@`, `&`, `-`, `.`, `:` and `?`. <br>

Remove also non-informative words: `resolution`, `rt`, `2015` and the empty word. <br>

Plot the top $100$ remaining words in a word cloud, using the `wordcloud2` package. <br>





e. (8pt) Find for each of the top (most frequent) $100$ words from 2.(d.) and each of the $10$ tweet categories, the fraction of tweets from this category where the word appears, and list them in a $100 \times 10$ table $F$, with $f_{ij}$ indicating the frequency of word $i$ in category $j$. <br>

That is, if for example there were $200$ tweets in the category `Humor`, and $30$ of them contained the word `joke`, then the frequency was $0.15$. <br>

Finally, for each of the $10$ categories we want to find the most `characteristic` words, i.e. words appearing more frequently in this category compared to other categories: <br>

Formally, compute for each word $i$ and each category $j$ the difference between the frequency in the category and the maximum over frequencies in other categories: $d_{ij} = f_{ij} - \max_{k \neq j} f_{ik}$.

(For example, if the word `joke` had frequency $0.15$ in `Humor`, and the next highest frequency for this word in other categories is $0.1$, then the difference for this word is $0.05$).

Find for each category $j$ of the $10$ categories the $3$ `characteristic` words with the highest differences $d_{ij}$. Show a table with the $10$ categories and the $3$ `characteristic` words you have found for each of them. Do the words make sense for the categories? 





f. (5pt) Plot the number of tweets in each of the $10$ categories shown in `Resolution_Category`. <br>

Next, compute and show in a table of size $10 \times 4$ the number of tweets for each of the $10$ categories from users in each of the four regions of the USA: `Midwest`, `Northeast`, `South` and `West`. 







g. (8pt) We want to test the null hypothesis that users in different `regions`  have the same distribution over `categories` for their resolutions, using the Pearson chi-square statistic: 

$$

S = \sum_{i=1}^{10} \sum_{j=1}^{4} \frac{(o_{ij}-e_{ij})^2}{e_{ij}}

$$

where $o_{ij}$ is the number of tweets on category $i$ from region $j$ computed in the table in the previous sub-question, assuming some indexing for the categories and regions (for example, $j=1,2,3,4$ for `Midwest`, `Northeast`, `South` and `West`, respectively, and similarly for the categories). The expected counts $e_{ij}$ are given by: 

$$

e_{ij} = \frac{o_{ \bullet j} o_{i \bullet}  }  {o_{\bullet \bullet}}

$$

where $o_{i \bullet}$ is the sum over the $i$'th row (over all regions), $o_{\bullet j}$  the sum over the $j$'th column (over all categories) and $o_{\bullet \bullet}$ the sum over all observations in the table. These expected counts correspond to independence between the row (categories) and column (regions) according to the null hypothesis. <br>

Compute and report the test statistic for the table computed in 2.(f). <br>

Use the approximation $S \sim \chi^2(27)$ to compute a p-value for the above test (there are $(4-1) \times (10-1) = 27$ degrees of freedom). Would you reject the null hypothesis? <br>

Finally, repeat the analysis (computing a table, $\chi^2$-statistic and p-value) but this time split tweets by `gender` (`male` and `female`) instead of by `region`, to get a $10 \times 2$ table. Is there a significant difference in the distribution of categories between males and females?





h. (8pt) Use the following simulation to create a randomized dataset of `(category, region)` pairs for the tweets: <br>

For each tweet in the dataset keep the real `category` (from the column `Resolution_Category`) but change the `region` randomly by shuffling (permuting) the regions column in a random order, such that the total number of tweets from each region remains the same. <br>

Repeat this simulation $N=1,000$ times, each time creating a new shuffled random data, with the `category` column remaining the same and the `region` column shuffled each time in a random order. 

For each such simulation indexed $i$ compute the `category`-by-`region` occurance table and the resulting $\chi^2$ test statistic from 2.(g.) and call it $S_i$. <br>

Plot the empirical density distribution of the $S_i$ randomized test statistics and compare it to the theoretical density of the $\chi^2(27)$ distribution. Are the distributions similar? <br>

Finally, compute the empirical p-value, comparing the test statistic $S$ computed on the real data in 2.(g.) to the $1,000$ random statistics:  

$$

\widehat{Pval} = \frac{1}{N} \sum_{i=1}^N 1_{\{S_i \geq S\}}.

$$

How different from the p-value obtained via the chi-square approximation? 





i. (8pt) Compute for each of the $50$ states (and `DC` - District of Columbia) in the US the number of tweets made by users from this state. <br>

Next, load the `usmap` library that contains the variable `statepop`. <br>

Use this variable to compute the number of tweets per million residents for each state. <br>

Remove `DC` and use the `usmap` package to make a map of USA states, where each state is colored by the number of tweets per million residents. <br>

Report the three states with the maximal and minimal number. 







**Solutions:**



## Q2. Analysis and Visualization of Twitter Data (60 pt)    

a. (4pt) Download and read the tweets dataset file `New-years-resolutions-DFE.csv`.


```r
data.new_years <- data.frame(read.csv("New-years-resolutions-DFE.csv"))
attach(data.new_years)
class(text) # Checking if it is character type.
```

```
## [1] "character"
```

Show the top and bottom two rows of the resulting data-frame. 

```r
knitr::kable(head(data.new_years,2))
```



|other_topic                           |resolution_topics                                     |gender |name         |Resolution_Category | retweet_count|text                                                                                                                                     |tweet_coord |tweet_created  |tweet_date |           tweet_id|tweet_location      |tweet_state |user_timezone              |tweet_region |
|:-------------------------------------|:-----------------------------------------------------|:------|:------------|:-------------------|-------------:|:----------------------------------------------------------------------------------------------------------------------------------------|:-----------|:--------------|:----------|------------------:|:-------------------|:-----------|:--------------------------|:------------|
|Read moore books, read less facebook. |Eat healthier                                         |female |Dena_Marina  |Health & Fitness    |             0|#NewYearsResolution :: Read more books, No scrolling FB/checking email b4 breakfast, stay dedicated to PT/yoga to squash my achin' back! |            |12/31/14 10:48 |12/31/14   | 550363000000000000|Southern California |CA          |Pacific Time (US & Canada) |West         |
|                                      |Humor about Personal Growth and Interests Resolutions |female |ninjagirl325 |Humor               |             1|#NewYearsResolution Finally master @ZJ10 's part of Kitchen Sink                                                                         |            |12/31/14 10:47 |12/31/14   | 550363000000000000|New Jersey          |NJ          |Central Time (US & Canada) |Northeast    |

```r
knitr::kable(tail(data.new_years,2))
```



|     |other_topic          |resolution_topics |gender |name         |Resolution_Category | retweet_count|text                                                                                                                                           |tweet_coord |tweet_created |tweet_date |           tweet_id|tweet_location          |tweet_state |user_timezone              |tweet_region |
|:----|:--------------------|:-----------------|:------|:------------|:-------------------|-------------:|:----------------------------------------------------------------------------------------------------------------------------------------------|:-----------|:-------------|:----------|------------------:|:-----------------------|:-----------|:--------------------------|:------------|
|5010 |                     |Join a startup    |female |itsmeJajael  |Career              |            NA|RT @kscmaghirang: To have an excellent job before or after graduation #NewYearsResolution                                                      |            |12/31/14 9:48 |12/31/14   | 550348000000000000|Paris  USA              |TX          |Beijing                    |South        |
|5011 |humor on resolutions |Improve my body   |female |_LeahHarrell |Health & Fitness    |            NA|RT @tompycan: #NewYearsResolution on Jan1: "I'm really going to get in shape this year!"

on Jan3: "I'm learning to love my body the way itâ€°<db>_ |            |12/31/14 9:51 |12/31/14   | 550348000000000000|shenandoah conservatory |VA          |Eastern Time (US & Canada) |South        |

b. (5pt) Create a new column with tweet times, of class `times`, with the time of the day for each tweet, in the above format. For example, the first entry in the column corresponding to the time of the first tweet should be: `10:48:00`.


Make a histogram showing the number of tweets in every hour of the $24$ hours in a day (that is, the bins are times between `00:00` and `00:59`, between `01:00` and `01:59` etc.). <br>

At which hours do we see the most/fewest tweets?

a new column with tweet times, of class `times`.

```r
data.new_years <- mutate(data.new_years, time_created = tweet_created) # new column time

data.new_years$time_created <- as.chron(data.new_years$time_created, "%m/%d/%Y %H:%M") # using chron library
data.new_years$time_created <- format(data.new_years$time_created, format = "%H:%M:%S") # save as a format
data.new_years$time_created <- as.times(data.new_years$time_created )

class(data.new_years$time_created) # time class
```

```
## [1] "times"
```

Make a histogram showing the number of tweets in every hour of the $24$ hours in a day. 

```r
hist(hours(data.new_years$time_created), breaks=24, main="2.b number of tweets in every hour in a day.",ylab = "Tweets rate", xlab = "Hours" ,col='#65BDED', border="white", font.axis=2 )
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-24-1.png" width="672" />

At which hours do we see the most/fewest tweets? It is easy to notice that bin 9 has the largest number of tweets i.e. at $8:00-9:00$ this is the time with the most tweets.

c. (6pt) Plot the distribution of tweets `text` lengths (in characters) made by `females` and `males` separately. Who writes longer tweets?

```r
data.new_years <- mutate(data.new_years, length_text = nchar(text)) # a new column of text lengths.

ggplot(data = data.new_years,aes(x=gender,y = length_text, col=gender))+
  geom_boxplot()+
  theme_light()+
  theme(legend.position="none") +
  xlab("gender")+
  ylab("tweets text lengths")+
  coord_flip()+
  labs(title="2.c The distribution for the different gender")
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-25-1.png" width="672" />

It is easy to see that on average women tweets are longer in length than men.

plot the tweets lengths distribution for tweets in the four different regions of the US.

```r
ggplot(data = data.new_years,aes(x=tweet_region,y = length_text, col=tweet_region))+
  geom_boxplot()+
  theme_light()+
  theme(legend.position="none") +
  xlab("region")+
  ylab("tweets text lengths")+
  coord_flip()+
  labs(title="2.c The distribution for the different region")
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-26-1.png" width="672" />

It is easy to see that on average south and northeast tweets in equal and greater lengths than in the rest.
But northeast holds greater variance than south.

tweets lengths distribution for tweets for the $10$ different categories given in `Resolution_Category`.

```r
ggplot(data = data.new_years,aes(x=Resolution_Category,y = length_text, col=Resolution_Category))+
  geom_boxplot()+
  theme_light()+
  theme(legend.position="none") +
  xlab("Category")+
  ylab("tweets text lengths")+
  coord_flip()+
  labs(title="2.c The distribution for the different categories.")
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-27-1.png" width="672" />

It is easy to see that the length of the text in Philanthropic is greater than the rest, since its average is higher than the rest. But we can notice that there is more variation from the average towards shorter texts.


d. (8pt) Compute the number of occurrences of each word in the `text` of all the tweets. Ignore upper/lower case differences.

```r
text <- sort(table(unlist(strsplit(tolower(data.new_years$text), " "))), decreasing = TRUE) # as assignment 5
count_words <- setNames(data.frame(text), c("word", "count")) %>% anti_join(stop_words, by = "word")
count_words <- count_words[!str_detect(pattern =  "#|@|&|\\.|-|:|\\?|\\s|\\d|\\!", string = count_words$word),] # remove hashtags, names
count_words <- count_words %>% filter(!count_words$word %in% c("resolution","rt","2015"))
count_words$word <-  str_replace_all(count_words$word,"[^[:alnum:]]", "") # remove symbols as ",/" in words.

count_words <- count_words[!apply(count_words, 1, function(x) any(x=="")),] 
wordcloud2(data = count_words[1:100,], size = 0.7, shape = 'triangle-forward', gridSize = 10)
```

<!--html_preserve--><div id="htmlwidget-e930dcd2b734c64d6df2" style="width:672px;height:480px;" class="wordcloud2 html-widget"></div>
<script type="application/json" data-for="htmlwidget-e930dcd2b734c64d6df2">{"x":{"word":["stop","time","start","people","eat","life","happy","love","day","learn","gonna","years","drink","gym","stay","continue","lose","fuck","smoking","read","resolve","friends","meet","money","spend","write","finally","watch","year","goal","change","live","tweet","more","eating","resolutions","finish","quit","club","hope","lol","supper","buy","starting","twitter","weight","food","mine","focus","living","play","leave","shit","care","days","person","plan","wanna","family","resolution","social","wear","healthy","save","wait","gain","positive","ready","ass","book","giving","hard","taking","workout","bad","daily","drinking","follow","goals","lot","week","break","im","set","thinking","win","everyday","fit","grow","job","bed","decided","game","god","hate","healthier","school","cut","fucking","gotta"],"freq":[348,180,173,165,128,107,98,94,92,88,86,78,66,65,65,60,60,58,58,56,55,53,52,51,51,49,48,47,47,45,44,44,44,43,42,41,40,40,38,38,38,38,37,36,36,36,35,35,34,34,34,32,31,30,30,30,30,30,29,29,29,29,28,28,28,27,27,27,26,26,26,26,26,26,25,25,25,25,25,25,25,24,24,24,24,24,23,23,23,23,22,22,22,22,22,22,22,21,21,21],"fontFamily":"Segoe UI","fontWeight":"bold","color":"random-dark","minSize":0,"weightFactor":0.362068965517241,"backgroundColor":"white","gridSize":10,"minRotation":-0.785398163397448,"maxRotation":0.785398163397448,"shuffle":true,"rotateRatio":0.4,"shape":"triangle-forward","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":{"render":[{"code":"function(el,x){\n                        console.log(123);\n                        if(!iii){\n                          window.location.reload();\n                          iii = False;\n\n                        }\n  }","data":null}]}}</script><!--/html_preserve-->

e. (8pt) Find for each of the top (most frequent) $100$ words from 2.(d.) and each of the $10$ tweet categories, the fraction of tweets from this category where the word appears, and list them in a $100 \times 10$ table $F$, with $f_{ij}$ indicating the frequency of word $i$ in category $j$.

```r
sum_Category <- data.new_years %>% group_by(Resolution_Category) %>% summarise(number = n())
```

```
## `summarise()` ungrouping output (override with `.groups` argument)
```

```r
data.2e <- data.frame(matrix(NA, nrow =100, ncol = 10)) # Create a table 100*10
data.2e <- setNames(data.2e, as.vector(sum_Category$Resolution_Category)) # insert category name.
rownames(data.2e) <- as.vector(count_words$word[1:100]) # insert words in column.

# insert the how appears for word in categore.
for(i in 1:10){for(j in 1:100){conrtr = 0 #Condition check if this is the word is in the tweet and also the word and category
    for(k in 1:5011){if((grepl(row.names(data.2e)[j], data.new_years[k,7], fixed = TRUE)) & (data.new_years[k,5] == colnames(data.2e)[i])){ conrtr = conrtr+1}}
  data.2e[j,i] <- conrtr/sum_Category[i,2]}}


data.2e.t <- data.frame(t(data.2e)) # Transfer to df for convenience
data.2e.t <- data.2e.t %>% summarise_if(is.numeric, max) # Finding a maximum for each category
data.2e.t <- t(data.2e.t) # Back to subtract value from the maximum

difference <- abs(data.2e-data.2e.t)
for(i in 1:10){print(knitr::kable(top_n(difference,3, difference[i])[i]))} # for testing, the common words
```

```
## 
## 
## |      |    Career|
## |:-----|---------:|
## |learn | 0.2359551|
## |money | 0.5147908|
## |more  | 0.2378247|
## 
## 
## |      | Education/Training|
## |:-----|------------------:|
## |money |          0.5002554|
## |more  |          0.3245020|
## |job   |          0.1633672|
## 
## 
## |      | Family/Friends/Relationships|
## |:-----|----------------------------:|
## |learn |                    0.2302571|
## |money |                    0.5227273|
## |more  |                    0.2239867|
## 
## 
## |      |   Finance|
## |:-----|---------:|
## |learn | 0.2245914|
## |im    | 0.1554990|
## |job   | 0.1689214|
## 
## 
## |      | Health & Fitness|
## |:-----|----------------:|
## |learn |        0.2276217|
## |money |        0.5167749|
## |more  |        0.2544913|
## 
## 
## |      |     Humor|
## |:-----|---------:|
## |learn | 0.2270442|
## |money | 0.5148065|
## |more  | 0.2480086|
## 
## 
## |      | Personal Growth|
## |:-----|---------------:|
## |learn |       0.2202336|
## |money |       0.5176739|
## |more  |       0.1970771|
## 
## 
## |      | Philanthropic|
## |:-----|-------------:|
## |learn |     0.2121455|
## |money |     0.5227273|
## |job   |     0.1626984|
## 
## 
## |      | Recreation & Leisure|
## |:-----|--------------------:|
## |learn |            0.2252484|
## |money |            0.5205859|
## |job   |            0.1724618|
## 
## 
## |      | Time Management/Organization|
## |:-----|----------------------------:|
## |learn |                    0.2244608|
## |money |                    0.4997388|
## |more  |                    0.1852795|
```

```r
# the words with multiple shows in the category
diff <- (data.2e-data.2e.t)
for(i in 1:10){print(knitr::kable(top_n(diff,3, diff[i])[i]))}
```

```
## 
## 
## |       | Career|
## |:------|------:|
## |start  |      0|
## |stay   |      0|
## |write  |      0|
## |goal   |      0|
## |hope   |      0|
## |supper |      0|
## |focus  |      0|
## |care   |      0|
## |wanna  |      0|
## |wait   |      0|
## |hard   |      0|
## |goals  |      0|
## |set    |      0|
## |job    |      0|
## 
## 
## |         | Education/Training|
## |:--------|------------------:|
## |learn    |                  0|
## |continue |                  0|
## |finally  |                  0|
## |supper   |                  0|
## |mine     |                  0|
## |days     |                  0|
## |ass      |                  0|
## |giving   |                  0|
## |school   |                  0|
## |fucking  |                  0|
## 
## 
## |        | Family/Friends/Relationships|
## |:-------|----------------------------:|
## |people  |                            0|
## |love    |                            0|
## |friends |                            0|
## |meet    |                            0|
## |supper  |                            0|
## |family  |                            0|
## |social  |                            0|
## |follow  |                            0|
## |cut     |                            0|
## 
## 
## |       | Finance|
## |:------|-------:|
## |money  |       0|
## |spend  |       0|
## |more   |       0|
## |supper |       0|
## |buy    |       0|
## |food   |       0|
## |save   |       0|
## |taking |       0|
## |win    |       0|
## |god    |       0|
## 
## 
## |          | Health & Fitness|
## |:---------|----------------:|
## |stop      |                0|
## |eat       |                0|
## |day       |                0|
## |drink     |                0|
## |gym       |                0|
## |lose      |                0|
## |smoking   |                0|
## |eating    |                0|
## |quit      |                0|
## |club      |                0|
## |supper    |                0|
## |starting  |                0|
## |weight    |                0|
## |healthy   |                0|
## |gain      |                0|
## |workout   |                0|
## |drinking  |                0|
## |week      |                0|
## |break     |                0|
## |fit       |                0|
## |healthier |                0|
## 
## 
## |           | Humor|
## |:----------|-----:|
## |fuck       |     0|
## |resolve    |     0|
## |lol        |     0|
## |supper     |     0|
## |resolution |     0|
## |bed        |     0|
## 
## 
## |         | Personal Growth|
## |:--------|---------------:|
## |life     |               0|
## |happy    |               0|
## |year     |               0|
## |supper   |               0|
## |living   |               0|
## |person   |               0|
## |positive |               0|
## 
## 
## |         | Philanthropic|
## |:--------|-------------:|
## |gonna    |             0|
## |change   |             0|
## |live     |             0|
## |tweet    |             0|
## |supper   |             0|
## |plan     |             0|
## |daily    |             0|
## |thinking |             0|
## |grow     |             0|
## |hate     |             0|
## 
## 
## |        | Recreation & Leisure|
## |:-------|--------------------:|
## |watch   |                    0|
## |supper  |                    0|
## |play    |                    0|
## |decided |                    0|
## |game    |                    0|
## 
## 
## |            | Time Management/Organization|
## |:-----------|----------------------------:|
## |time        |                            0|
## |years       |                            0|
## |read        |                            0|
## |resolutions |                            0|
## |finish      |                            0|
## |supper      |                            0|
## |twitter     |                            0|
## |leave       |                            0|
## |shit        |                            0|
## |wear        |                            0|
## |ready       |                            0|
## |book        |                            0|
## |bad         |                            0|
## |lot         |                            0|
## |im          |                            0|
## |everyday    |                            0|
## |gotta       |                            0|
```

Since we were asked for the most distance, 
defined words are not common to the same category - it can be seen that indeed the words we found are not meaningful in the categories in which they are. Unlike the more common words do indeed make much sense in the category in which they are. I checked to know that this is the case.

f. Plot the number of tweets in each of the categories shown in Resolution_Category.

```r
ggplot(sum_Category, aes(x=Resolution_Category, y=number))+ theme_light() +
  geom_bar(color='#65BDED', fill='#65BDED',stat = "identity")+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) +xlab("")+ ylab("number of appearances")+ labs(title="2.f. number of tweets in each of the categories.")
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-30-1.png" width="672" />

Category personal received the most tweets among the categories.

compute and show in a table of size $10 \times 4$

```r
data_region <- table(Resolution_Category,tweet_region) # by table we can show 
knitr::kable(data_region,caption= "Category & region")
```



Table: Category & region

|                             | Midwest| Northeast| South| West|
|:----------------------------|-------:|---------:|-----:|----:|
|Career                       |      24|        30|    44|   28|
|Education/Training           |      14|        19|    33|   23|
|Family/Friends/Relationships |      72|        79|    97|  103|
|Finance                      |      38|        32|    65|   41|
|Health & Fitness             |     191|       170|   284|  195|
|Humor                        |     201|       226|   300|  283|
|Personal Growth              |     352|       375|   592|  462|
|Philanthropic                |      12|        19|    34|   19|
|Recreation & Leisure         |      96|       101|   146|  124|
|Time Management/Organization |      20|        22|    26|   19|

g. Is there a significant difference in the distribution of categories between males and females?

```r
chisq.test.gender <- chisq.test(data_region)
chisq.test.gender
```

```
## 
## 	Pearson's Chi-squared test
## 
## data:  data_region
## X-squared = 26.366, df = 27, p-value = 0.4984
```
$P-value >> 0.1$
It can be noted that the p-value of the quadratic live test is greater than alpha equal to 0.05 or 0.1 therefore i will not reject the null hypothesis.


```r
data_gender  <- table(Resolution_Category,gender)
knitr::kable(data_gender,caption= "Category & gender")
```



Table: Category & gender

|                             | female| male|
|:----------------------------|------:|----:|
|Career                       |     46|   80|
|Education/Training           |     44|   45|
|Family/Friends/Relationships |    188|  163|
|Finance                      |     96|   80|
|Health & Fitness             |    467|  373|
|Humor                        |    369|  641|
|Personal Growth              |    975|  806|
|Philanthropic                |     42|   42|
|Recreation & Leisure         |    216|  251|
|Time Management/Organization |     50|   37|


```r
chisq.test(data_gender)
```

```
## 
## 	Pearson's Chi-squared test
## 
## data:  data_gender
## X-squared = 116.67, df = 9, p-value < 0.00000000000000022
```
$P-value << 0.05$
It can be noted that the p-value of the live test in a square, is small and the $\alpha$ water is equal to 0.05 or 0.1 and even an excess is smaller than that. We will therefore reject the null hypothesis.

Is there a significant difference in the distribution of categories between males and females?  As I wrote, in gender the null hypothesis can be rejected. Which in the categories the hypothesis cannot be rejected.

h.(8pt) Use the following simulation to create a randomized dataset of (category, region) pairs for the tweets:

```r
random.ex2.h <- function(){region <- tweet_region
  df <- data.new_years
  for(i in seq(length(rownames(data.new_years)))){
    random <- rdunif(1,1,length(region))
    df[i,15] <- region[random]
    regions <- region[-random]}
  randoms <- table(df$Resolution_Category,df$tweet_region)
  return(chisq.test(randoms)$statistic)}

res.ex2.h <- c()
res.ex2.h = sapply(1:1000, function(index){res.ex2.h[index] <- random.ex2.h()})
```

Plot the empirical density distribution of the randomized test statistics.

```r
ggplot(data = as.data.frame(res.ex2.h),aes(x=res.ex2.h))+
  geom_density(fill='#65BDED')+  theme_light() + stat_function(fun = dchisq, args = list(df = 27),col='red') + geom_text(y=.055, x=45, label="red line is \n dchisq Distribution df=27 ",size = 3.5, color = "red") + 
  geom_text(x=45, y=0.045, label="black line is \n simulation Distribution df=27",size = 3.5, color = "black") +xlab("dchisq simulation Distribution")
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-36-1.png" width="672" />

It can be shown that they are indeed very similar and that if we run a larger number of iterations, it will definitely be concentrated there according to the law of large numbers.

compute the empirical p-value, comparing the test statistic $S$ computed on the real data in 2.(g.) to the 1,000 random statistics?

```r
p_value.hut <- length(res.ex2.h[res.ex2.h>=chisq.test.gender$statistic])/1000
p_value.hut
```

```
## [1] 0.504
```
p-value came out lower than in a standard Chi-squared test calculation the different.

```r
different <- p_value.hut - chisq.test.gender$p.value
different
```

```
## [1] 0.005613929
```

Since I accepted that the estimate for p-value is greater than 0.05 or 0.1 then I did not reject the null hypothesis.
p-value came out lower than in a standard Chi-squared test calculation the different.

i.(8pt) Compute for each of the states (and DC - District of Columbia) in the US the number of tweets made by users from this state.

```r
us.states <- data.new_years %>% count(tweet_state) # count tweet in each stat
state.p.p.million <- statepop$pop_2015/10^6 # Percentage of people per million
colnames(us.states) <- c("abbr", "number")
join_data <- full_join(statepop, us.states, by="abbr")
join_data <- mutate(join_data, per = join_data$number/state.p.p.million) # a new column of text lengths.

plot_usmap(data = join_data,values = "per", exclude = "DC", color = "red", labels = TRUE)+scale_fill_continuous(name = "tweet per million", label = scales::comma)+ theme(legend.position = "right")
```

<img src="52414-HomeExam_314696972_files/figure-html/unnamed-chunk-39-1.png" width="672" />

Report the three states with the maximal and minimal number.

```r
knitr::kable(top_n(join_data,3, per)[2:3], caption = "top 3 states")
```



Table: top 3 states

|abbr |full                 |
|:----|:--------------------|
|AK   |Alaska               |
|DC   |District of Columbia |
|NY   |New York             |


```r
min_3 <- tail(join_data[order(join_data[6],decreasing = TRUE),],3) # Ordering by Value
rownames(min_3) <-NULL
knitr::kable(min_3[2:3], caption = "minimum 3 of states")
```



Table: minimum 3 of states

|abbr |full         |
|:----|:------------|
|DE   |Delaware     |
|MT   |Montana      |
|ND   |North Dakota |

